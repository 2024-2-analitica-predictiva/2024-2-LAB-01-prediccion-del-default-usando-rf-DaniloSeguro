{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:\n",
      "   LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
      "0     310000    1          3         1   32      0      0      0      0   \n",
      "1      10000    2          3         1   49     -1     -1     -2     -1   \n",
      "2      50000    1          2         1   28     -1     -1     -1      0   \n",
      "3      80000    2          3         1   52      2      2      3      3   \n",
      "4     270000    1          1         2   34      1      2      0      0   \n",
      "\n",
      "   PAY_5  ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  \\\n",
      "0      0  ...      84373      57779      14163      8295      6000      4000   \n",
      "1      2  ...       1690       1138        930         0         0      2828   \n",
      "2     -1  ...      45975       1300      43987         0     46257      2200   \n",
      "3      3  ...      40748      39816      40607      3700      1600      1600   \n",
      "4      2  ...      22448      15490      17343         0      4000      2000   \n",
      "\n",
      "   PAY_AMT4  PAY_AMT5  PAY_AMT6  default  \n",
      "0      3000      1000      2000        0  \n",
      "1         0       182         0        1  \n",
      "2      1300     43987      1386        0  \n",
      "3         0      1600      1600        1  \n",
      "4         0      2000      2000        0  \n",
      "\n",
      "[5 rows x 24 columns]\n",
      "Test Data:\n",
      "   LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
      "0     120000    2          2         2   26     -1      2      0      0   \n",
      "1      20000    1          3         2   35     -2     -2     -2     -2   \n",
      "2     200000    2          3         2   34      0      0      2      0   \n",
      "3     250000    1          1         2   29      0      0      0      0   \n",
      "4      50000    2          3         3   23      1      2      0      0   \n",
      "\n",
      "   PAY_5  ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  \\\n",
      "0      0  ...       3272       3455       3261         0      1000      1000   \n",
      "1     -1  ...          0      13007      13912         0         0         0   \n",
      "2      0  ...       2513       1828       3731      2306        12        50   \n",
      "3      0  ...      59696      56875      55512      3000      3000      3000   \n",
      "4      0  ...      28771      29531      30211         0      1500      1100   \n",
      "\n",
      "   PAY_AMT4  PAY_AMT5  PAY_AMT6  default  \n",
      "0      1000         0      2000        1  \n",
      "1     13007      1122         0        0  \n",
      "2       300      3738        66        0  \n",
      "3      3000      3000      3000        0  \n",
      "4      1200      1300      1100        0  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "# Importar librerías necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Definir rutas\n",
    "train_file_path = r\"C:\\Users\\danil\\Documents\\GitHub2\\2024-2-LAB-01-prediccion-del-default-usando-rf-DaniloSeguro\\files\\input\\train_data.csv.zip\"\n",
    "test_file_path = r\"C:\\Users\\danil\\Documents\\GitHub2\\2024-2-LAB-01-prediccion-del-default-usando-rf-DaniloSeguro\\files\\input\\test_data.csv.zip\"\n",
    "\n",
    "# Verificar si los archivos existen\n",
    "if not os.path.exists(train_file_path):\n",
    "    print(f\"El archivo {train_file_path} no existe. Por favor, asegúrese de que el archivo esté en la ubicación correcta.\")\n",
    "if not os.path.exists(test_file_path):\n",
    "    print(f\"El archivo {test_file_path} no existe. Por favor, asegúrese de que el archivo esté en la ubicación correcta.\")\n",
    "\n",
    "# Función para cargar y limpiar los datos\n",
    "def load_and_clean_data(file_path):\n",
    "    with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"files/input/\")\n",
    "        extracted_file = zip_ref.namelist()[0]\n",
    "    \n",
    "    # Leer el dataset\n",
    "    df = pd.read_csv(f\"files/input/{extracted_file}\")\n",
    "    \n",
    "    # Renombrar columna objetivo\n",
    "    df.rename(columns={\"default payment next month\": \"default\"}, inplace=True)\n",
    "    \n",
    "    # Eliminar columna 'ID'\n",
    "    df.drop(columns=[\"ID\"], inplace=True)\n",
    "    \n",
    "    # Eliminar registros con información no disponible (NaN)\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # Agrupar valores > 4 de 'EDUCATION' en \"others\"\n",
    "    df['EDUCATION'] = df['EDUCATION'].apply(lambda x: 4 if x > 4 else x)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Cargar y limpiar datos\n",
    "train_df = load_and_clean_data(train_file_path)\n",
    "test_df = load_and_clean_data(test_file_path)\n",
    "\n",
    "# Revisar datasets limpios\n",
    "print(\"Train Data:\")\n",
    "print(train_df.head())\n",
    "print(\"Test Data:\")\n",
    "print(test_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (21000, 23), y_train shape: (21000,)\n",
      "X_test shape: (9000, 23), y_test shape: (9000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separar variables explicativas (X) y objetivo (y)\n",
    "X_train = train_df.drop(columns=[\"default\"])\n",
    "y_train = train_df[\"default\"]\n",
    "\n",
    "X_test = test_df.drop(columns=[\"default\"])\n",
    "y_test = test_df[\"default\"]\n",
    "\n",
    "# Verificar las dimensiones\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline creado y modelo entrenado correctamente.\n"
     ]
    }
   ],
   "source": [
    "# Importar librerías necesarias\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Identificar columnas categóricas y numéricas\n",
    "categorical_features = ['SEX', 'EDUCATION', 'MARRIAGE']  # Variables categóricas\n",
    "numerical_features = [col for col in X_train.columns if col not in categorical_features]\n",
    "\n",
    "# Crear transformador para codificación One-Hot de variables categóricas\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Crear el preprocesador usando ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'  # Mantener variables numéricas sin cambios\n",
    ")\n",
    "\n",
    "# Crear el pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Entrenar el modelo en el conjunto de entrenamiento\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Imprimir mensaje de éxito\n",
    "print(\"Pipeline creado y modelo entrenado correctamente.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 81 candidates, totalling 810 fits\n",
      "Mejores hiperparámetros encontrados:\n",
      "{'classifier__max_depth': None, 'classifier__min_samples_leaf': 2, 'classifier__min_samples_split': 10, 'classifier__n_estimators': 300}\n",
      "Precisión del modelo optimizado en el conjunto de prueba: 0.8292\n"
     ]
    }
   ],
   "source": [
    "# Importar librerías necesarias\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, balanced_accuracy_score\n",
    "\n",
    "# Definir los hiperparámetros a optimizar\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [100, 200, 300],   # Número de árboles en el bosque\n",
    "    'classifier__max_depth': [None, 10, 20],       # Profundidad máxima de los árboles\n",
    "    'classifier__min_samples_split': [2, 5, 10],   # Mínimo número de muestras para dividir un nodo\n",
    "    'classifier__min_samples_leaf': [1, 2, 4],     # Mínimo número de muestras en una hoja\n",
    "}\n",
    "\n",
    "# Crear un scorer para la precisión balanceada\n",
    "scorer = make_scorer(balanced_accuracy_score)\n",
    "\n",
    "# Configurar la validación cruzada\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=10,                          # Número de folds para validación cruzada\n",
    "    scoring=scorer,                 # Métrica de precisión balanceada\n",
    "    n_jobs=-1,                      # Usar todos los núcleos disponibles\n",
    "    verbose=2                       # Mostrar el progreso\n",
    ")\n",
    "\n",
    "# Ejecutar la optimización de hiperparámetros\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Mostrar los mejores hiperparámetros encontrados\n",
    "print(\"Mejores hiperparámetros encontrados:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Asignar el mejor modelo a una variable\n",
    "best_model = grid_search.best_estimator_hgb\n",
    "\n",
    "# Imprimir el rendimiento en el conjunto de prueba\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "print(f\"Precisión del modelo optimizado en el conjunto de prueba: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado correctamente en: files/models/model.pkl.gz\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import gzip\n",
    "import os\n",
    "\n",
    "# Definir la ruta correcta usando os.path.join\n",
    "output_dir = \"files/models\"\n",
    "os.makedirs(output_dir, exist_ok=True)  # Crear el directorio si no existe\n",
    "model_path = os.path.join(output_dir, \"model.pkl.gz\")\n",
    "\n",
    "# Asegurar que las rutas tengan slashes correctos\n",
    "model_path = model_path.replace(\"\\\\\", \"/\")  # Reemplazar backslashes con slashes\n",
    "\n",
    "# Guardar el modelo comprimido usando gzip\n",
    "with gzip.open(model_path, 'wb') as f:\n",
    "    joblib.dump(best_model, f)\n",
    "\n",
    "print(f\"Modelo guardado correctamente en: {model_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas de entrenamiento:\n",
      "{\n",
      "  \"dataset\": \"train\",\n",
      "  \"precision\": 0.9581774906494389,\n",
      "  \"balanced_accuracy\": 0.7942956226666198,\n",
      "  \"recall\": 0.5961497778718003,\n",
      "  \"f1_score\": 0.7350026082420449\n",
      "}\n",
      "\n",
      "Métricas de prueba:\n",
      "{\n",
      "  \"dataset\": \"test\",\n",
      "  \"precision\": 0.6600688468158348,\n",
      "  \"balanced_accuracy\": 0.6730383115731366,\n",
      "  \"recall\": 0.4017810371922472,\n",
      "  \"f1_score\": 0.4995115597525236\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Importar librerías necesarias\n",
    "from sklearn.metrics import precision_score, balanced_accuracy_score, recall_score, f1_score\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Crear una función para calcular las métricas\n",
    "def calculate_metrics(model, X, y, dataset_name):\n",
    "    y_pred = model.predict(X)\n",
    "    metrics = {\n",
    "        'dataset': dataset_name,\n",
    "        'precision': precision_score(y, y_pred),\n",
    "        'balanced_accuracy': balanced_accuracy_score(y, y_pred),\n",
    "        'recall': recall_score(y, y_pred),\n",
    "        'f1_score': f1_score(y, y_pred)\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# Calcular métricas para entrenamiento y prueba\n",
    "train_metrics = calculate_metrics(best_model, X_train, y_train, 'train')\n",
    "test_metrics = calculate_metrics(best_model, X_test, y_test, 'test')\n",
    "\n",
    "# Crear directorio de salida si no existe\n",
    "output_dir = \"files/output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Guardar las métricas en un archivo JSON\n",
    "metrics_path = os.path.join(output_dir, \"metrics.json\")\n",
    "\n",
    "# mostrar las metricas\n",
    "print(\"Métricas de entrenamiento:\")\n",
    "print(json.dumps(train_metrics, indent=2))\n",
    "print(\"\\nMétricas de prueba:\")\n",
    "print(json.dumps(test_metrics, indent=2))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrices de confusión calculadas y añadidas:\n",
      "[\n",
      "    {\n",
      "        \"type\": \"cm_matrix\",\n",
      "        \"dataset\": \"train\",\n",
      "        \"true_0\": {\n",
      "            \"predicted_0\": 16150,\n",
      "            \"predicted_1\": 123\n",
      "        },\n",
      "        \"true_1\": {\n",
      "            \"predicted_0\": 1909,\n",
      "            \"predicted_1\": 2818\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"type\": \"cm_matrix\",\n",
      "        \"dataset\": \"test\",\n",
      "        \"true_0\": {\n",
      "            \"predicted_0\": 6696,\n",
      "            \"predicted_1\": 395\n",
      "        },\n",
      "        \"true_1\": {\n",
      "            \"predicted_0\": 1142,\n",
      "            \"predicted_1\": 767\n",
      "        }\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Importar librerías necesarias\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Función para calcular la matriz de confusión y formatearla\n",
    "def get_confusion_matrix(model, X, y, dataset_name):\n",
    "    y_pred = model.predict(X)\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    \n",
    "    cm_dict = {\n",
    "        \"type\": \"cm_matrix\",\n",
    "        \"dataset\": dataset_name,\n",
    "        \"true_0\": {\"predicted_0\": int(cm[0, 0]), \"predicted_1\": int(cm[0, 1])},\n",
    "        \"true_1\": {\"predicted_0\": int(cm[1, 0]), \"predicted_1\": int(cm[1, 1])}\n",
    "    }\n",
    "    return cm_dict\n",
    "\n",
    "# Calcular matrices de confusión para entrenamiento y prueba\n",
    "train_cm = get_confusion_matrix(best_model, X_train, y_train, \"train\")\n",
    "test_cm = get_confusion_matrix(best_model, X_test, y_test, \"test\")\n",
    "\n",
    "# Cargar las métricas existentes o crear una lista vacía si el archivo no existe\n",
    "if os.path.exists(metrics_path):\n",
    "    with open(metrics_path, 'r') as f:\n",
    "        metrics = json.load(f)\n",
    "else:\n",
    "    metrics = []\n",
    "\n",
    "# Actualizar las métricas con las matrices de confusión\n",
    "metrics.append(train_cm)\n",
    "metrics.append(test_cm)\n",
    "\n",
    "# Guardar las métricas actualizadas en el archivo JSON\n",
    "with open(metrics_path, 'w') as f:\n",
    "    json.dump(metrics, f, indent=4)\n",
    "\n",
    "# Mostrar las matrices de confusión\n",
    "print(\"Matrices de confusión calculadas y añadidas:\")\n",
    "print(json.dumps([train_cm, test_cm], indent=4))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
